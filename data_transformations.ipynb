{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9e5d302",
   "metadata": {},
   "source": [
    "# Data Transformations\n",
    "\n",
    "This notebook contains code to:\n",
    "- read data about trending USA videos from initial dataset\n",
    "- transform dataset using NLTK and TextVectorization and split it into train and test sets\n",
    "- create pretrained Embedding layers\n",
    "- save all the necessary information for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad07b35",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b480d0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4d07bd",
   "metadata": {},
   "source": [
    "## Download nltk data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abd156c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Dmytro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to use nltk.corpus.words.words()\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd338209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dmytro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to use nltk.tokenize.word_tokenize(string)\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e838b63f",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "446300f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ids(filename):\n",
    "    \"\"\" Read ids from file to list \"\"\"\n",
    "\n",
    "    with open(filename, 'r') as f:\n",
    "        ids = f.read().splitlines()\n",
    "\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75edc665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorizer(text, max_tokens=10000, sentence_length=1000):\n",
    "    \"\"\" \n",
    "    Create TextVectorization() layer based on text\n",
    "\n",
    "    Parameters:\n",
    "        text (array of strings): text\n",
    "        max_tokens (int): size of the vocabulary.\n",
    "            The layer will consider only 'max_tokens' top popular words\n",
    "        sentence_length (int): maximum number of words in a sentence.\n",
    "            Longer sentences will be truncate, shorter ones will be pad\n",
    "\n",
    "    Returns:\n",
    "        vectorizer layer (tf.keras.layers.TextVectorization)\n",
    "    \"\"\"\n",
    "\n",
    "    # create a vectorizer object\n",
    "    vectorizer = layers.TextVectorization(max_tokens=max_tokens,\n",
    "                                          output_sequence_length=sentence_length)\n",
    "    vectorizer.adapt(text)\n",
    "\n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af2eba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(vocabulary,\n",
    "                            path_to_emb_file='embeddings/glove.6B/glove.6B.50d.txt',\n",
    "                            emb_dim=50):\n",
    "    \"\"\" \n",
    "    Create an embedding matrix based on pretrained GloVe embeddings\n",
    "    and a vocabulary obtained by TextVectorization layer\n",
    "    Adapted from: https://keras.io/examples/nlp/pretrained_word_embeddings/\n",
    "\n",
    "    Parameters:\n",
    "        path_to_emb_file (str): path to word embeddings .txt file\n",
    "        vocabulary (list): vocabulary obtained by TextVectorization with get_vocabulary().\n",
    "            The list of 'max_tokens' top popular words from the text\n",
    "        emb_dim (int): embedding dimensions. \n",
    "            In the case of GloVe can be 50, 100, 200 or 300\n",
    "\n",
    "    Returns:\n",
    "        embedding_matrix with shape=(num_tokens, emb_dim) \n",
    "        where num_tokens = len(vocabulary) + 2 (two extra tokens - pad and OOV)\n",
    "    \"\"\"\n",
    "\n",
    "    # Read embedding text file\n",
    "    try:\n",
    "        f = open(path_to_emb_file, encoding=\"utf8\")\n",
    "    except OSError:\n",
    "        print(f\"Could not open/read file: {path_to_emb_file}.\")\n",
    "        return None\n",
    "\n",
    "    embeddings_index = {}\n",
    "    with f:\n",
    "        for line in f:\n",
    "            word, coefs = line.split(maxsplit=1)\n",
    "            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "            embeddings_index[word] = coefs\n",
    "    print(f\"Found {len(embeddings_index)} word vectors.\")\n",
    "\n",
    "    # +2 for padding and OOV (out of vocabulary) tokens\n",
    "    num_tokens = len(vocabulary) + 2\n",
    "    word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "    embedding_dim = emb_dim\n",
    "    hits = 0\n",
    "    misses = 0\n",
    "\n",
    "    # Create and fill embedding matrix\n",
    "    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            # This includes the representation for \"padding\" and \"OOV\"\n",
    "            misses += 1\n",
    "    print(f\"Converted {hits} words ({misses} misses)\")\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3fb04cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_embedding_layer(emb_matrix, name=''):\n",
    "    \"\"\" \n",
    "    Create a non-trainable Embedding layer initialized with 'emb_matrix' \n",
    "\n",
    "    Parameters:\n",
    "        emb_matrix (matrix of shape (num_tokens, embedding_dim)): embedding matrix\n",
    "        name (str): name of the layer\n",
    "\n",
    "    Returns:\n",
    "        embedding layer (tf.keras.layers.Embedding)\n",
    "    \"\"\"\n",
    "\n",
    "    num_tokens, embedding_dim = emb_matrix.shape\n",
    "\n",
    "    embedding_layer = layers.Embedding(\n",
    "        num_tokens,\n",
    "        embedding_dim,\n",
    "        embeddings_initializer=keras.initializers.Constant(emb_matrix),\n",
    "        trainable=False,\n",
    "        name=name)\n",
    "\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "669e407c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(vocabulary, name='',\n",
    "                               path_to_emb_file='embeddings/glove.6B/glove.6B.50d.txt',\n",
    "                               emb_dim=50):\n",
    "    \"\"\" \n",
    "    Combine create_embedding_matrix() and init_embedding_layer() functions\n",
    "    to create non-trainable Embedding layer based on vocabulary and \n",
    "    word embeddings\n",
    "\n",
    "    Parameters:\n",
    "        vocabulary (list): vocabulary obtained by TextVectorization with get_vocabulary().\n",
    "            The list of 'max_tokens' top popular words from the text\n",
    "        name (str): name of the layer\n",
    "        path_to_emb_file (str): path to word embeddings .txt file\n",
    "        emb_dim (int): embedding dimensions. \n",
    "            In the case of GloVe can be 50, 100, 200 or 300\n",
    "\n",
    "    Returns:\n",
    "        embedding layer (tf.keras.layers.Embedding)\n",
    "    \"\"\"\n",
    "\n",
    "    embedding_matrix = create_embedding_matrix(\n",
    "        vocabulary=vocabulary,\n",
    "        path_to_emb_file=path_to_emb_file,\n",
    "        emb_dim=emb_dim)\n",
    "\n",
    "    embedding_layer = init_embedding_layer(embedding_matrix, name=name)\n",
    "\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee5c7ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_numeric(X):\n",
    "    \"\"\" Slice numerical columns from a DataFrame and convert them to numpy \"\"\"\n",
    "    \n",
    "    numeric_data = X[['view_count', 'likes', 'comment_count']].to_numpy()\n",
    "    return numeric_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8399ad64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(series, max_tokens, sentence_length, test=False, vectorizer=None):\n",
    "    \"\"\" \n",
    "    Preprocess text data.\n",
    "    Convert a series of text data into its vectorized form\n",
    "\n",
    "    Parameters:\n",
    "        series (pd.Series): dataframe column with text\n",
    "        max_tokens and sentence_length (ints): see create_vectorizer() function\n",
    "        test (bool): if False - vectorizer adapted with text.\n",
    "                     if True, vectorizer must be specified\n",
    "        vectorizer (tf.keras.layers.TextVectorization) - vectorizer adapted to text.\n",
    "            Must be specified if test=True\n",
    "            \n",
    "    Returns:\n",
    "        vectorizer (tf.keras.layers.TextVectorization)\n",
    "        text_data - tensor with shape (n, sentence_length)\n",
    "    \"\"\"\n",
    "    \n",
    "    text = series.to_numpy()\n",
    "    \n",
    "    if not test:\n",
    "        vectorizer = create_vectorizer(text, max_tokens, sentence_length)\n",
    "        vocabulary = vectorizer.get_vocabulary()\n",
    "\n",
    "    text_data = vectorizer(text)\n",
    "\n",
    "    return vectorizer, text_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316611c8",
   "metadata": {},
   "source": [
    "# Dataset Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1999a75",
   "metadata": {},
   "source": [
    "## Read and split dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "275c2d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>title</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>channel_title</th>\n",
       "      <th>published_at</th>\n",
       "      <th>view_count</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>tags</th>\n",
       "      <th>description</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--0bCF-iK2E</td>\n",
       "      <td>Jadon Sancho  Magical Skills &amp; Goals</td>\n",
       "      <td>UC6UL29enLNe4mqwTfAyeNuw</td>\n",
       "      <td>Bundesliga</td>\n",
       "      <td>2021-07-01 10:00:00</td>\n",
       "      <td>1048888</td>\n",
       "      <td>19515</td>\n",
       "      <td>226</td>\n",
       "      <td>1319</td>\n",
       "      <td>football soccer ftbol alemn Bundesliga season ...</td>\n",
       "      <td>Enjoy the best skills and goals from Jadon San...</td>\n",
       "      <td>Respect to Dortmund fans,must be sad losing hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--14w5SOEUs</td>\n",
       "      <td>Migos - Avalanche (Official Video)</td>\n",
       "      <td>UCGIelM2Dj3zza3xyV3pL3WQ</td>\n",
       "      <td>MigosVEVO</td>\n",
       "      <td>2021-06-10 16:00:00</td>\n",
       "      <td>15352638</td>\n",
       "      <td>359277</td>\n",
       "      <td>7479</td>\n",
       "      <td>18729</td>\n",
       "      <td>Migos Avalanche Quality Control Music/Motown R...</td>\n",
       "      <td>Watch the the official video for Migos - \"Aval...</td>\n",
       "      <td>Migos just makes me want to live my live to th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>--40TEbZ9Is</td>\n",
       "      <td>Supporting Actress in a Comedy: 73rd Emmys</td>\n",
       "      <td>UClBKH8yZRcM4AsRjDVEdjMg</td>\n",
       "      <td>Television Academy</td>\n",
       "      <td>2021-09-20 01:03:32</td>\n",
       "      <td>925281</td>\n",
       "      <td>11212</td>\n",
       "      <td>401</td>\n",
       "      <td>831</td>\n",
       "      <td></td>\n",
       "      <td>Hannah Waddingham wins the Emmy for Supporting...</td>\n",
       "      <td>Hannah's energy bursts through any screen. Wel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>--4tfbSyYDE</td>\n",
       "      <td>JO1'YOUNG (JO1 ver.)' PERFORMANCE VIDEO</td>\n",
       "      <td>UCsmXiDP8S40uBeJYxvyulmA</td>\n",
       "      <td>JO1</td>\n",
       "      <td>2021-03-03 10:00:17</td>\n",
       "      <td>2641597</td>\n",
       "      <td>39131</td>\n",
       "      <td>441</td>\n",
       "      <td>3745</td>\n",
       "      <td>PRODUCE101JAPAN              JO1   TheSTAR STA...</td>\n",
       "      <td>JO1'YOUNG (JO1 ver.)' PERFORMANCE VIDEO\\n\\n---...</td>\n",
       "      <td>youngVer&gt;&lt;  REN is really PERFECT. It's not ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>--DKkzWVh-E</td>\n",
       "      <td>Why Retaining Walls Collapse</td>\n",
       "      <td>UCMOqf8ab-42UUQIdVoKwjlQ</td>\n",
       "      <td>Practical Engineering</td>\n",
       "      <td>2021-12-07 13:00:00</td>\n",
       "      <td>715724</td>\n",
       "      <td>32887</td>\n",
       "      <td>367</td>\n",
       "      <td>1067</td>\n",
       "      <td>retaining wall New Jersey highway Direct Conne...</td>\n",
       "      <td>One of the most important (and innocuous) part...</td>\n",
       "      <td>Keep up with all my projects here: https://pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37417</th>\n",
       "      <td>zzd4ydafGR0</td>\n",
       "      <td>Lil Tjay - Calling My Phone (feat. 6LACK) [Off...</td>\n",
       "      <td>UCEB4a5o_6KfjxHwNMnmj54Q</td>\n",
       "      <td>Lil Tjay</td>\n",
       "      <td>2021-02-12 05:03:49</td>\n",
       "      <td>120408275</td>\n",
       "      <td>2180780</td>\n",
       "      <td>35871</td>\n",
       "      <td>81360</td>\n",
       "      <td>Lil Tjay Steady Calling My Phone Calling My Ph...</td>\n",
       "      <td>Official video for \"Calling My Phone\" by Lil T...</td>\n",
       "      <td>'DESTINED 2 WIN' OUT NOW !! https://liltjay.ln...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37418</th>\n",
       "      <td>zziBybeSAtw</td>\n",
       "      <td>PELICANS at LAKERS | FULL GAME HIGHLIGHTS | Ja...</td>\n",
       "      <td>UCWJ2lWNubArHWmf3FIHbfcQ</td>\n",
       "      <td>NBA</td>\n",
       "      <td>2021-01-16 05:39:05</td>\n",
       "      <td>2841917</td>\n",
       "      <td>20759</td>\n",
       "      <td>1049</td>\n",
       "      <td>2624</td>\n",
       "      <td>NBA G League Basketball game-0022000187 Lakers...</td>\n",
       "      <td>PELICANS at LAKERS | FULL GAME HIGHLIGHTS | Ja...</td>\n",
       "      <td>Montrezl Harrell is going crazy with the rebou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37419</th>\n",
       "      <td>zzk09ESX7e0</td>\n",
       "      <td>[MV]  (MAMAMOO) - Where Are We Now</td>\n",
       "      <td>UCuhAUMLzJxlP1W7mEk0_6lA</td>\n",
       "      <td>MAMAMOO</td>\n",
       "      <td>2021-06-02 09:00:10</td>\n",
       "      <td>13346678</td>\n",
       "      <td>720854</td>\n",
       "      <td>4426</td>\n",
       "      <td>90616</td>\n",
       "      <td>MAMAMOO  WAW  WAW MAMAMOO WAW Where Are We Now...</td>\n",
       "      <td>[MV]  (MAMAMOO) - Where Are We Now\\n\\nInstagra...</td>\n",
       "      <td>I honestly do not know why this song hit so ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37420</th>\n",
       "      <td>zzmQEb0Em5I</td>\n",
       "      <td>FELLIPE ESCUDERO- Master Podcast  #12</td>\n",
       "      <td>UC8NjnNWMsRqq11NYvHAQb1g</td>\n",
       "      <td>Master Podcast</td>\n",
       "      <td>2020-10-20 20:59:30</td>\n",
       "      <td>252057</td>\n",
       "      <td>19198</td>\n",
       "      <td>1234</td>\n",
       "      <td>1471</td>\n",
       "      <td>master masterpodcast lord lord vinheteiro z z ...</td>\n",
       "      <td>DOCTOR HAIR\\nhttps://www.thedoctorhair.com/?fb...</td>\n",
       "      <td>Foi um prazer passar esta tarde com vocs debat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37421</th>\n",
       "      <td>zzxPZwaA-8w</td>\n",
       "      <td>Gareth Bale brace secures dramatic comeback on...</td>\n",
       "      <td>UCEg25rdRZXg32iwai6N6l0w</td>\n",
       "      <td>Tottenham Hotspur</td>\n",
       "      <td>2021-05-23 21:00:31</td>\n",
       "      <td>2252090</td>\n",
       "      <td>34063</td>\n",
       "      <td>868</td>\n",
       "      <td>2004</td>\n",
       "      <td>Spurs Tottenham Hotspur   Tottenham Leicester ...</td>\n",
       "      <td>Two minute highlights from Tottenham Hotspur's...</td>\n",
       "      <td>Thank you Kane for everything you have given t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37422 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          video_id                                              title  \\\n",
       "0      --0bCF-iK2E               Jadon Sancho  Magical Skills & Goals   \n",
       "1      --14w5SOEUs                 Migos - Avalanche (Official Video)   \n",
       "2      --40TEbZ9Is         Supporting Actress in a Comedy: 73rd Emmys   \n",
       "3      --4tfbSyYDE            JO1'YOUNG (JO1 ver.)' PERFORMANCE VIDEO   \n",
       "4      --DKkzWVh-E                       Why Retaining Walls Collapse   \n",
       "...            ...                                                ...   \n",
       "37417  zzd4ydafGR0  Lil Tjay - Calling My Phone (feat. 6LACK) [Off...   \n",
       "37418  zziBybeSAtw  PELICANS at LAKERS | FULL GAME HIGHLIGHTS | Ja...   \n",
       "37419  zzk09ESX7e0                 [MV]  (MAMAMOO) - Where Are We Now   \n",
       "37420  zzmQEb0Em5I              FELLIPE ESCUDERO- Master Podcast  #12   \n",
       "37421  zzxPZwaA-8w  Gareth Bale brace secures dramatic comeback on...   \n",
       "\n",
       "                     channel_id          channel_title         published_at  \\\n",
       "0      UC6UL29enLNe4mqwTfAyeNuw             Bundesliga  2021-07-01 10:00:00   \n",
       "1      UCGIelM2Dj3zza3xyV3pL3WQ              MigosVEVO  2021-06-10 16:00:00   \n",
       "2      UClBKH8yZRcM4AsRjDVEdjMg     Television Academy  2021-09-20 01:03:32   \n",
       "3      UCsmXiDP8S40uBeJYxvyulmA                    JO1  2021-03-03 10:00:17   \n",
       "4      UCMOqf8ab-42UUQIdVoKwjlQ  Practical Engineering  2021-12-07 13:00:00   \n",
       "...                         ...                    ...                  ...   \n",
       "37417  UCEB4a5o_6KfjxHwNMnmj54Q               Lil Tjay  2021-02-12 05:03:49   \n",
       "37418  UCWJ2lWNubArHWmf3FIHbfcQ                    NBA  2021-01-16 05:39:05   \n",
       "37419  UCuhAUMLzJxlP1W7mEk0_6lA                MAMAMOO  2021-06-02 09:00:10   \n",
       "37420  UC8NjnNWMsRqq11NYvHAQb1g         Master Podcast  2020-10-20 20:59:30   \n",
       "37421  UCEg25rdRZXg32iwai6N6l0w      Tottenham Hotspur  2021-05-23 21:00:31   \n",
       "\n",
       "       view_count    likes  dislikes  comment_count  \\\n",
       "0         1048888    19515       226           1319   \n",
       "1        15352638   359277      7479          18729   \n",
       "2          925281    11212       401            831   \n",
       "3         2641597    39131       441           3745   \n",
       "4          715724    32887       367           1067   \n",
       "...           ...      ...       ...            ...   \n",
       "37417   120408275  2180780     35871          81360   \n",
       "37418     2841917    20759      1049           2624   \n",
       "37419    13346678   720854      4426          90616   \n",
       "37420      252057    19198      1234           1471   \n",
       "37421     2252090    34063       868           2004   \n",
       "\n",
       "                                                    tags  \\\n",
       "0      football soccer ftbol alemn Bundesliga season ...   \n",
       "1      Migos Avalanche Quality Control Music/Motown R...   \n",
       "2                                                          \n",
       "3      PRODUCE101JAPAN              JO1   TheSTAR STA...   \n",
       "4      retaining wall New Jersey highway Direct Conne...   \n",
       "...                                                  ...   \n",
       "37417  Lil Tjay Steady Calling My Phone Calling My Ph...   \n",
       "37418  NBA G League Basketball game-0022000187 Lakers...   \n",
       "37419  MAMAMOO  WAW  WAW MAMAMOO WAW Where Are We Now...   \n",
       "37420  master masterpodcast lord lord vinheteiro z z ...   \n",
       "37421  Spurs Tottenham Hotspur   Tottenham Leicester ...   \n",
       "\n",
       "                                             description  \\\n",
       "0      Enjoy the best skills and goals from Jadon San...   \n",
       "1      Watch the the official video for Migos - \"Aval...   \n",
       "2      Hannah Waddingham wins the Emmy for Supporting...   \n",
       "3      JO1'YOUNG (JO1 ver.)' PERFORMANCE VIDEO\\n\\n---...   \n",
       "4      One of the most important (and innocuous) part...   \n",
       "...                                                  ...   \n",
       "37417  Official video for \"Calling My Phone\" by Lil T...   \n",
       "37418  PELICANS at LAKERS | FULL GAME HIGHLIGHTS | Ja...   \n",
       "37419  [MV]  (MAMAMOO) - Where Are We Now\\n\\nInstagra...   \n",
       "37420  DOCTOR HAIR\\nhttps://www.thedoctorhair.com/?fb...   \n",
       "37421  Two minute highlights from Tottenham Hotspur's...   \n",
       "\n",
       "                                                comments  \n",
       "0      Respect to Dortmund fans,must be sad losing hi...  \n",
       "1      Migos just makes me want to live my live to th...  \n",
       "2      Hannah's energy bursts through any screen. Wel...  \n",
       "3      youngVer><  REN is really PERFECT. It's not ju...  \n",
       "4       Keep up with all my projects here: https://pr...  \n",
       "...                                                  ...  \n",
       "37417  'DESTINED 2 WIN' OUT NOW !! https://liltjay.ln...  \n",
       "37418  Montrezl Harrell is going crazy with the rebou...  \n",
       "37419  I honestly do not know why this song hit so ha...  \n",
       "37420  Foi um prazer passar esta tarde com vocs debat...  \n",
       "37421  Thank you Kane for everything you have given t...  \n",
       "\n",
       "[37422 rows x 12 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = 'datasets/youtube_custom_dataset/data/youtube_dislike_dataset.csv'\n",
    "\n",
    "df = pd.read_csv(dataset_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "640dde1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>title</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>channel_title</th>\n",
       "      <th>published_at</th>\n",
       "      <th>view_count</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>tags</th>\n",
       "      <th>description</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--14w5SOEUs</td>\n",
       "      <td>Migos - Avalanche (Official Video)</td>\n",
       "      <td>UCGIelM2Dj3zza3xyV3pL3WQ</td>\n",
       "      <td>MigosVEVO</td>\n",
       "      <td>2021-06-10 16:00:00</td>\n",
       "      <td>15352638</td>\n",
       "      <td>359277</td>\n",
       "      <td>7479</td>\n",
       "      <td>18729</td>\n",
       "      <td>Migos Avalanche Quality Control Music/Motown R...</td>\n",
       "      <td>Watch the the official video for Migos - \"Aval...</td>\n",
       "      <td>Migos just makes me want to live my live to th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--40TEbZ9Is</td>\n",
       "      <td>Supporting Actress in a Comedy: 73rd Emmys</td>\n",
       "      <td>UClBKH8yZRcM4AsRjDVEdjMg</td>\n",
       "      <td>Television Academy</td>\n",
       "      <td>2021-09-20 01:03:32</td>\n",
       "      <td>925281</td>\n",
       "      <td>11212</td>\n",
       "      <td>401</td>\n",
       "      <td>831</td>\n",
       "      <td></td>\n",
       "      <td>Hannah Waddingham wins the Emmy for Supporting...</td>\n",
       "      <td>Hannah's energy bursts through any screen. Wel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>--DKkzWVh-E</td>\n",
       "      <td>Why Retaining Walls Collapse</td>\n",
       "      <td>UCMOqf8ab-42UUQIdVoKwjlQ</td>\n",
       "      <td>Practical Engineering</td>\n",
       "      <td>2021-12-07 13:00:00</td>\n",
       "      <td>715724</td>\n",
       "      <td>32887</td>\n",
       "      <td>367</td>\n",
       "      <td>1067</td>\n",
       "      <td>retaining wall New Jersey highway Direct Conne...</td>\n",
       "      <td>One of the most important (and innocuous) part...</td>\n",
       "      <td>Keep up with all my projects here: https://pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>--FmExEAsM8</td>\n",
       "      <td>IVE  'ELEVEN' MV</td>\n",
       "      <td>UCYDmx2Sfpnaxg488yBpZIGg</td>\n",
       "      <td>starshipTV</td>\n",
       "      <td>2021-12-01 09:00:03</td>\n",
       "      <td>36124750</td>\n",
       "      <td>965069</td>\n",
       "      <td>16618</td>\n",
       "      <td>59657</td>\n",
       "      <td>Kpop girl group 1theK Starshiptv starship   MV...</td>\n",
       "      <td>IVE Twitter\\n: https://twitter.com/IVEstarship...</td>\n",
       "      <td>omg !! they are so young and......pretty !!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0PZSxZuAXQ</td>\n",
       "      <td>The Breakfast Club Reacts to Kanye West's \"DON...</td>\n",
       "      <td>UChi08h4577eFsNXGd3sxYhw</td>\n",
       "      <td>Breakfast Club Power 105.1 FM</td>\n",
       "      <td>2021-08-06 12:10:25</td>\n",
       "      <td>535044</td>\n",
       "      <td>9207</td>\n",
       "      <td>384</td>\n",
       "      <td>1900</td>\n",
       "      <td>the breakfast club breakfast club power1051 ce...</td>\n",
       "      <td>Subscribe NOW to The Breakfast Club: http://ih...</td>\n",
       "      <td>No one can create excitement like Kanye. Not e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15830</th>\n",
       "      <td>zyY6G1UX7HU</td>\n",
       "      <td>The Gaines' Living Room Renovation is Complete...</td>\n",
       "      <td>UC95x-m2toVa_BRmmQ6SrIYg</td>\n",
       "      <td>Magnolia Network</td>\n",
       "      <td>2021-02-11 21:36:15</td>\n",
       "      <td>547119</td>\n",
       "      <td>4014</td>\n",
       "      <td>136</td>\n",
       "      <td>188</td>\n",
       "      <td>magnolia chip gaines chip and jo waco magnolia...</td>\n",
       "      <td>Jo gives a tour of everyone's new favorite roo...</td>\n",
       "      <td>Absolutely beautiful!  Home is where the heart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15831</th>\n",
       "      <td>zyrJ0j7x9-4</td>\n",
       "      <td>FlightReacts Plays NBA 2K21 For The FIRST Time...</td>\n",
       "      <td>UCoGIPQ7M4NWai7LRgRhaSOg</td>\n",
       "      <td>NotYourAverageFlight</td>\n",
       "      <td>2020-08-25 03:45:13</td>\n",
       "      <td>1405603</td>\n",
       "      <td>44261</td>\n",
       "      <td>1522</td>\n",
       "      <td>4419</td>\n",
       "      <td>NBA 2K20 2K NBA NBA 2K20 NBA 2K20 BEST BUILD B...</td>\n",
       "      <td>SMUSH THAT LIKE IF YOU ENJOYED! \\n\\nLETS GET 3...</td>\n",
       "      <td>I like how flight is being considerate of othe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15832</th>\n",
       "      <td>zzd4ydafGR0</td>\n",
       "      <td>Lil Tjay - Calling My Phone (feat. 6LACK) [Off...</td>\n",
       "      <td>UCEB4a5o_6KfjxHwNMnmj54Q</td>\n",
       "      <td>Lil Tjay</td>\n",
       "      <td>2021-02-12 05:03:49</td>\n",
       "      <td>120408275</td>\n",
       "      <td>2180780</td>\n",
       "      <td>35871</td>\n",
       "      <td>81360</td>\n",
       "      <td>Lil Tjay Steady Calling My Phone Calling My Ph...</td>\n",
       "      <td>Official video for \"Calling My Phone\" by Lil T...</td>\n",
       "      <td>'DESTINED 2 WIN' OUT NOW !! https://liltjay.ln...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15833</th>\n",
       "      <td>zziBybeSAtw</td>\n",
       "      <td>PELICANS at LAKERS | FULL GAME HIGHLIGHTS | Ja...</td>\n",
       "      <td>UCWJ2lWNubArHWmf3FIHbfcQ</td>\n",
       "      <td>NBA</td>\n",
       "      <td>2021-01-16 05:39:05</td>\n",
       "      <td>2841917</td>\n",
       "      <td>20759</td>\n",
       "      <td>1049</td>\n",
       "      <td>2624</td>\n",
       "      <td>NBA G League Basketball game-0022000187 Lakers...</td>\n",
       "      <td>PELICANS at LAKERS | FULL GAME HIGHLIGHTS | Ja...</td>\n",
       "      <td>Montrezl Harrell is going crazy with the rebou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15834</th>\n",
       "      <td>zzk09ESX7e0</td>\n",
       "      <td>[MV]  (MAMAMOO) - Where Are We Now</td>\n",
       "      <td>UCuhAUMLzJxlP1W7mEk0_6lA</td>\n",
       "      <td>MAMAMOO</td>\n",
       "      <td>2021-06-02 09:00:10</td>\n",
       "      <td>13346678</td>\n",
       "      <td>720854</td>\n",
       "      <td>4426</td>\n",
       "      <td>90616</td>\n",
       "      <td>MAMAMOO  WAW  WAW MAMAMOO WAW Where Are We Now...</td>\n",
       "      <td>[MV]  (MAMAMOO) - Where Are We Now\\n\\nInstagra...</td>\n",
       "      <td>I honestly do not know why this song hit so ha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15835 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          video_id                                              title  \\\n",
       "0      --14w5SOEUs                 Migos - Avalanche (Official Video)   \n",
       "1      --40TEbZ9Is         Supporting Actress in a Comedy: 73rd Emmys   \n",
       "2      --DKkzWVh-E                       Why Retaining Walls Collapse   \n",
       "3      --FmExEAsM8                                   IVE  'ELEVEN' MV   \n",
       "4      -0PZSxZuAXQ  The Breakfast Club Reacts to Kanye West's \"DON...   \n",
       "...            ...                                                ...   \n",
       "15830  zyY6G1UX7HU  The Gaines' Living Room Renovation is Complete...   \n",
       "15831  zyrJ0j7x9-4  FlightReacts Plays NBA 2K21 For The FIRST Time...   \n",
       "15832  zzd4ydafGR0  Lil Tjay - Calling My Phone (feat. 6LACK) [Off...   \n",
       "15833  zziBybeSAtw  PELICANS at LAKERS | FULL GAME HIGHLIGHTS | Ja...   \n",
       "15834  zzk09ESX7e0                 [MV]  (MAMAMOO) - Where Are We Now   \n",
       "\n",
       "                     channel_id                  channel_title  \\\n",
       "0      UCGIelM2Dj3zza3xyV3pL3WQ                      MigosVEVO   \n",
       "1      UClBKH8yZRcM4AsRjDVEdjMg             Television Academy   \n",
       "2      UCMOqf8ab-42UUQIdVoKwjlQ          Practical Engineering   \n",
       "3      UCYDmx2Sfpnaxg488yBpZIGg                     starshipTV   \n",
       "4      UChi08h4577eFsNXGd3sxYhw  Breakfast Club Power 105.1 FM   \n",
       "...                         ...                            ...   \n",
       "15830  UC95x-m2toVa_BRmmQ6SrIYg               Magnolia Network   \n",
       "15831  UCoGIPQ7M4NWai7LRgRhaSOg           NotYourAverageFlight   \n",
       "15832  UCEB4a5o_6KfjxHwNMnmj54Q                       Lil Tjay   \n",
       "15833  UCWJ2lWNubArHWmf3FIHbfcQ                            NBA   \n",
       "15834  UCuhAUMLzJxlP1W7mEk0_6lA                        MAMAMOO   \n",
       "\n",
       "              published_at  view_count    likes  dislikes  comment_count  \\\n",
       "0      2021-06-10 16:00:00    15352638   359277      7479          18729   \n",
       "1      2021-09-20 01:03:32      925281    11212       401            831   \n",
       "2      2021-12-07 13:00:00      715724    32887       367           1067   \n",
       "3      2021-12-01 09:00:03    36124750   965069     16618          59657   \n",
       "4      2021-08-06 12:10:25      535044     9207       384           1900   \n",
       "...                    ...         ...      ...       ...            ...   \n",
       "15830  2021-02-11 21:36:15      547119     4014       136            188   \n",
       "15831  2020-08-25 03:45:13     1405603    44261      1522           4419   \n",
       "15832  2021-02-12 05:03:49   120408275  2180780     35871          81360   \n",
       "15833  2021-01-16 05:39:05     2841917    20759      1049           2624   \n",
       "15834  2021-06-02 09:00:10    13346678   720854      4426          90616   \n",
       "\n",
       "                                                    tags  \\\n",
       "0      Migos Avalanche Quality Control Music/Motown R...   \n",
       "1                                                          \n",
       "2      retaining wall New Jersey highway Direct Conne...   \n",
       "3      Kpop girl group 1theK Starshiptv starship   MV...   \n",
       "4      the breakfast club breakfast club power1051 ce...   \n",
       "...                                                  ...   \n",
       "15830  magnolia chip gaines chip and jo waco magnolia...   \n",
       "15831  NBA 2K20 2K NBA NBA 2K20 NBA 2K20 BEST BUILD B...   \n",
       "15832  Lil Tjay Steady Calling My Phone Calling My Ph...   \n",
       "15833  NBA G League Basketball game-0022000187 Lakers...   \n",
       "15834  MAMAMOO  WAW  WAW MAMAMOO WAW Where Are We Now...   \n",
       "\n",
       "                                             description  \\\n",
       "0      Watch the the official video for Migos - \"Aval...   \n",
       "1      Hannah Waddingham wins the Emmy for Supporting...   \n",
       "2      One of the most important (and innocuous) part...   \n",
       "3      IVE Twitter\\n: https://twitter.com/IVEstarship...   \n",
       "4      Subscribe NOW to The Breakfast Club: http://ih...   \n",
       "...                                                  ...   \n",
       "15830  Jo gives a tour of everyone's new favorite roo...   \n",
       "15831  SMUSH THAT LIKE IF YOU ENJOYED! \\n\\nLETS GET 3...   \n",
       "15832  Official video for \"Calling My Phone\" by Lil T...   \n",
       "15833  PELICANS at LAKERS | FULL GAME HIGHLIGHTS | Ja...   \n",
       "15834  [MV]  (MAMAMOO) - Where Are We Now\\n\\nInstagra...   \n",
       "\n",
       "                                                comments  \n",
       "0      Migos just makes me want to live my live to th...  \n",
       "1      Hannah's energy bursts through any screen. Wel...  \n",
       "2       Keep up with all my projects here: https://pr...  \n",
       "3      omg !! they are so young and......pretty !!!! ...  \n",
       "4      No one can create excitement like Kanye. Not e...  \n",
       "...                                                  ...  \n",
       "15830  Absolutely beautiful!  Home is where the heart...  \n",
       "15831  I like how flight is being considerate of othe...  \n",
       "15832  'DESTINED 2 WIN' OUT NOW !! https://liltjay.ln...  \n",
       "15833  Montrezl Harrell is going crazy with the rebou...  \n",
       "15834  I honestly do not know why this song hit so ha...  \n",
       "\n",
       "[15835 rows x 12 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read those lines whose ids were in the file\n",
    "# that is, those videos that were trending in the USA\n",
    "ids_US = read_ids(\n",
    "    \"datasets/youtube_custom_dataset/video_IDs/unique_ids_US.txt\")\n",
    "df = df[df['video_id'].isin(ids_US)]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86f56d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dmytro\\Anaconda3\\envs\\tenv\\lib\\site-packages\\pandas\\util\\_decorators.py:311: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# just in case, so that the TextVectorization doesn't raise exceptions\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0307db28",
   "metadata": {},
   "source": [
    "## Delete stop words and nonsence using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9152dfe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235892"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 235 thousand words that we will consider as words that have some meaning\n",
    "words = set(nltk.corpus.words.words())\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "581e9827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    \"\"\" Delete all words that are not contained in the 'words' set \"\"\"\n",
    "\n",
    "    return \" \".join(w.lower() for w in nltk.tokenize.word_tokenize(sentence)\n",
    "                    if w.lower() in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08de77d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-01f5ab705461>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['title_raw'] = df['title']\n",
      "<ipython-input-16-01f5ab705461>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['channel_title_raw'] = df['channel_title']\n",
      "<ipython-input-16-01f5ab705461>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['tags_raw'] = df['tags']\n",
      "<ipython-input-16-01f5ab705461>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['description_raw'] = df['description']\n",
      "<ipython-input-16-01f5ab705461>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['comments_raw'] = df['comments']\n"
     ]
    }
   ],
   "source": [
    "# save old versions of columns\n",
    "df['title_raw'] = df['title']\n",
    "df['channel_title_raw'] = df['channel_title']\n",
    "df['tags_raw'] = df['tags']\n",
    "df['description_raw'] = df['description']\n",
    "df['comments_raw'] = df['comments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58f99437",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-53634a5e3c1f>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['description'] = df['description_raw'].apply(clean_sentence)\n",
      "<ipython-input-17-53634a5e3c1f>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['comments'] = df['comments_raw'].apply(clean_sentence)\n"
     ]
    }
   ],
   "source": [
    "# delete specific words from 'description' and 'comments' columns\n",
    "\n",
    "# apply clean_sentence() function for every row\n",
    "# can take some time\n",
    "df['description'] = df['description_raw'].apply(clean_sentence)\n",
    "df['comments'] = df['comments_raw'].apply(clean_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6bf7cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-4140da6231f1>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['title'] = df['title_raw'].apply(remove_punctuation)\n",
      "<ipython-input-18-4140da6231f1>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['channel_title'] = df['channel_title_raw'].apply(remove_punctuation)\n",
      "<ipython-input-18-4140da6231f1>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['tags'] = df['tags_raw'].apply(remove_punctuation)\n"
     ]
    }
   ],
   "source": [
    "# delete punctuation from 'title', 'channel_title' and 'tags'\n",
    "\n",
    "def remove_punctuation(sentence):\n",
    "    \"\"\" Delete punctuation from sentence and convert words to lowercase \"\"\"\n",
    "\n",
    "    # string.isalnum() returns True if all the characters are alphanumeric, \n",
    "    # meaning alphabet letter (a-z) and numbers (0-9).\n",
    "    return \" \".join(w.lower() for w in nltk.tokenize.word_tokenize(sentence)\n",
    "                    if w.isalnum())\n",
    "\n",
    "\n",
    "df['title'] = df['title_raw'].apply(remove_punctuation)\n",
    "df['channel_title'] = df['channel_title_raw'].apply(remove_punctuation)\n",
    "df['tags'] = df['tags_raw'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35950eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-5149be577ac1>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['author_text'] = df[['title', 'channel_title',\n"
     ]
    }
   ],
   "source": [
    "# create one column that contains all text from author\n",
    "df['author_text'] = df[['title', 'channel_title',\n",
    "                        'tags', 'description']].agg(' '.join, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ef6963",
   "metadata": {},
   "source": [
    "## Final version of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e444e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset we will use\n",
    "df_final = df[['view_count', 'likes', 'comment_count', \n",
    "               'author_text', 'comments', 'dislikes']]\n",
    "\n",
    "X = df[['view_count', 'likes', 'comment_count', \n",
    "        'author_text', 'comments']]\n",
    "y = df['dislikes'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f84e95ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15818, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>view_count</th>\n",
       "      <th>likes</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>author_text</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15352638</td>\n",
       "      <td>359277</td>\n",
       "      <td>18729</td>\n",
       "      <td>migos avalanche official video migosvevo migos...</td>\n",
       "      <td>just me want to live my live to the the amount...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>925281</td>\n",
       "      <td>11212</td>\n",
       "      <td>831</td>\n",
       "      <td>supporting actress in a comedy 73rd emmys tele...</td>\n",
       "      <td>energy through any screen well deserved lookin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>715724</td>\n",
       "      <td>32887</td>\n",
       "      <td>1067</td>\n",
       "      <td>why retaining walls collapse practical enginee...</td>\n",
       "      <td>keep up with all my here get some free from wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36124750</td>\n",
       "      <td>965069</td>\n",
       "      <td>59657</td>\n",
       "      <td>ive mv starshiptv kpop girl group 1thek starsh...</td>\n",
       "      <td>they are so young and pretty me gust la un fan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>535044</td>\n",
       "      <td>9207</td>\n",
       "      <td>1900</td>\n",
       "      <td>the breakfast club reacts to kanye west donda ...</td>\n",
       "      <td>no one can create excitement like not even dra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15830</th>\n",
       "      <td>547119</td>\n",
       "      <td>4014</td>\n",
       "      <td>188</td>\n",
       "      <td>the gaines living room renovation is complete ...</td>\n",
       "      <td>absolutely beautiful home is where the heart i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15831</th>\n",
       "      <td>1405603</td>\n",
       "      <td>44261</td>\n",
       "      <td>4419</td>\n",
       "      <td>flightreacts plays nba 2k21 for the first time...</td>\n",
       "      <td>i like how flight is being considerate of othe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15832</th>\n",
       "      <td>120408275</td>\n",
       "      <td>2180780</td>\n",
       "      <td>81360</td>\n",
       "      <td>lil tjay calling my phone feat 6lack official ...</td>\n",
       "      <td>win out now what a song will be on repeat for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15833</th>\n",
       "      <td>2841917</td>\n",
       "      <td>20759</td>\n",
       "      <td>2624</td>\n",
       "      <td>pelicans at lakers full game highlights januar...</td>\n",
       "      <td>is going crazy with the and quickness and late...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15834</th>\n",
       "      <td>13346678</td>\n",
       "      <td>720854</td>\n",
       "      <td>90616</td>\n",
       "      <td>mv mamamoo where are we now mamamoo mamamoo wa...</td>\n",
       "      <td>i honestly do not know why this song hit so ha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15818 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       view_count    likes  comment_count  \\\n",
       "0        15352638   359277          18729   \n",
       "1          925281    11212            831   \n",
       "2          715724    32887           1067   \n",
       "3        36124750   965069          59657   \n",
       "4          535044     9207           1900   \n",
       "...           ...      ...            ...   \n",
       "15830      547119     4014            188   \n",
       "15831     1405603    44261           4419   \n",
       "15832   120408275  2180780          81360   \n",
       "15833     2841917    20759           2624   \n",
       "15834    13346678   720854          90616   \n",
       "\n",
       "                                             author_text  \\\n",
       "0      migos avalanche official video migosvevo migos...   \n",
       "1      supporting actress in a comedy 73rd emmys tele...   \n",
       "2      why retaining walls collapse practical enginee...   \n",
       "3      ive mv starshiptv kpop girl group 1thek starsh...   \n",
       "4      the breakfast club reacts to kanye west donda ...   \n",
       "...                                                  ...   \n",
       "15830  the gaines living room renovation is complete ...   \n",
       "15831  flightreacts plays nba 2k21 for the first time...   \n",
       "15832  lil tjay calling my phone feat 6lack official ...   \n",
       "15833  pelicans at lakers full game highlights januar...   \n",
       "15834  mv mamamoo where are we now mamamoo mamamoo wa...   \n",
       "\n",
       "                                                comments  \n",
       "0      just me want to live my live to the the amount...  \n",
       "1      energy through any screen well deserved lookin...  \n",
       "2      keep up with all my here get some free from wi...  \n",
       "3      they are so young and pretty me gust la un fan...  \n",
       "4      no one can create excitement like not even dra...  \n",
       "...                                                  ...  \n",
       "15830  absolutely beautiful home is where the heart i...  \n",
       "15831  i like how flight is being considerate of othe...  \n",
       "15832  win out now what a song will be on repeat for ...  \n",
       "15833  is going crazy with the and quickness and late...  \n",
       "15834  i honestly do not know why this song hit so ha...  \n",
       "\n",
       "[15818 rows x 5 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X.shape)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f034f0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15818,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 7479,   401,   367, ..., 35871,  1049,  4426], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y.shape)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4050bae",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6428ef29",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221678d8",
   "metadata": {},
   "source": [
    "## Preprocessing numeric data\n",
    "\n",
    "All we need to do - convert pd.DataFrame to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3218a865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14818, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1374717,   59169,    9040],\n",
       "       [ 139175,    1369,     427],\n",
       "       [2106358,   27141,    3702],\n",
       "       ...,\n",
       "       [ 680084,   27568,    1576],\n",
       "       [4386699,   60530,    1872],\n",
       "       [2726003,  129602,    9069]], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_data_train = preprocess_numeric(X_train)\n",
    "print(numeric_data_train.shape)\n",
    "numeric_data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86d482f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  638692,     5948,      481],\n",
       "       [  822619,    51189,     1901],\n",
       "       [ 7789702,   142901,     7902],\n",
       "       ...,\n",
       "       [  252019,     9103,     1234],\n",
       "       [30590495,   416719,    13204],\n",
       "       [29601239,   913667,    61624]], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_data_test = preprocess_numeric(X_test)\n",
    "print(numeric_data_test.shape)\n",
    "numeric_data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc0ae07",
   "metadata": {},
   "source": [
    "## Preprocessing text data\n",
    "\n",
    "1. Read text data from pd.Series to numpy array\n",
    "2. Create a vectorizer object that will convert text to vectors of fixed length\n",
    "3. Get vocabulary from the vectorizer. Vocabulary is a list of words, that appear in the text, it will be used to create an Embedding layer\n",
    "4. Transform text to vectors\n",
    "\n",
    "This needs to be done for `author_text` and `comments`, but with different objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ad6d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# выяснить sentence_length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a07889b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the vocabulary\n",
    "# The layer will consider only the 'max_tokens' top popular words\n",
    "max_tokens = 20000\n",
    "\n",
    "# Maximum number of words in a sentence\n",
    "# Longer sentences will be truncate, shorter ones will be pad\n",
    "sentence_length = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0dc3af",
   "metadata": {},
   "source": [
    "### author_text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e61f6027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([14818, 2000])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_author, author_text_data_train = preprocess_text(X_train['author_text'],\n",
    "                                                            max_tokens, sentence_length,\n",
    "                                                            test=False)\n",
    "author_text_data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75363e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1000, 2000])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, author_text_data_test = preprocess_text(X_test['author_text'],\n",
    "                                           max_tokens, sentence_length,\n",
    "                                           test=True, vectorizer=vectorizer_author)\n",
    "author_text_data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7859240",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_author = vectorizer_author.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61804aa0",
   "metadata": {},
   "source": [
    "### comments_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ffc8f973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([14818, 2000])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_comments, comments_data_train = preprocess_text(X_train['comments'],\n",
    "                                                           max_tokens, sentence_length,\n",
    "                                                           test=False)\n",
    "comments_data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2d6474e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1000, 2000])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, comments_data_test = preprocess_text(X_test['comments'],\n",
    "                                        max_tokens, sentence_length,\n",
    "                                        test=True, vectorizer=vectorizer_comments)\n",
    "comments_data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9e6ff4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_comments = vectorizer_comments.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d4a094e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# without preprocessing function to see shapes\n",
    "\n",
    "# comments = X_train['comments'].to_numpy()\n",
    "# comments.shape\n",
    "\n",
    "# vectorizer_comments = create_vectorizer(comments, max_tokens, sentence_length)\n",
    "# vocabulary_comments = vectorizer_comments.get_vocabulary()\n",
    "# author_text_data = vectorizer_author(author_text)\n",
    "# author_text_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eeb21f",
   "metadata": {},
   "source": [
    "## Create pre-trained Endebbings layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6746158f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "Converted 17199 words (2801 misses)\n"
     ]
    }
   ],
   "source": [
    "embeddings_1 = pretrained_embedding_layer(vocabulary_author,\n",
    "                                          name='pretrained_embeddings_for_author_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de6c8293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "Converted 19535 words (465 misses)\n"
     ]
    }
   ],
   "source": [
    "embeddings_2 = pretrained_embedding_layer(vocabulary_comments,\n",
    "                                          name='pretrained_embeddings_for_comments')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8648005c",
   "metadata": {},
   "source": [
    "# Saving data to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0e91ba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = 'models/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f62c5106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset in csv format\n",
    "df_final.to_csv(f'{save_folder}/final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "144c0881",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'{save_folder}/y_train.npy', y_train)\n",
    "np.save(f'{save_folder}/y_test.npy', y_test)\n",
    "\n",
    "np.save(f'{save_folder}/numeric_data_train.npy', numeric_data_train)\n",
    "np.save(f'{save_folder}/numeric_data_test.npy', numeric_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1bfc4009",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(author_text_data_train, \n",
    "            open(f'{save_folder}/author_text_data_train.p', 'wb'))\n",
    "pickle.dump(author_text_data_test, \n",
    "            open(f'{save_folder}/author_text_data_test.p', 'wb'))\n",
    "\n",
    "pickle.dump(comments_data_train, \n",
    "            open(f'{save_folder}/comments_data_train.p', 'wb'))\n",
    "pickle.dump(comments_data_test, \n",
    "            open(f'{save_folder}/comments_data_test.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ba73203b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(embeddings_1, open(f'{save_folder}/embeddings_1.p', 'wb'))\n",
    "pickle.dump(embeddings_2, open(f'{save_folder}/embeddings_2.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "81302cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from numpy\n",
    "# var = np.load('filename.npy')\n",
    "\n",
    "# from pickle\n",
    "# var = pickle.load(open(\"filename.p\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ffec59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
